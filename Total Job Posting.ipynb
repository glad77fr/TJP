{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Power BI TS Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "import xlsxwriter\n",
    "import openpyxl # 3.0\n",
    "import re\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Déclaration de la classe Datamanagement et des fonctions associées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamanagement:\n",
    "    def __init__(self):\n",
    "        self.source = {} # Dictionnaire contenant le nom du fichier source et son contenu stocké dans un DataFrame\n",
    "        self.dim = {} # Dictionnaire contenant le nom et les dataframes des tables de dimension\n",
    "        self.fact = {} # Dictionnaire contenant le nom et les dataframes liés aux tables de faits\n",
    "        self.changed_keys = {}        \n",
    "        self.group_col = pd.DataFrame(columns=[\"Nom colonne\", \"Dimension cible\", \"Colonne cible\"], data=[]) # DataFrame contenant les colonnes de regroupement créés\n",
    "        try:\n",
    "            self.settings = pd.ExcelFile(r\"Settings & documentation\\Settings.xlsx\")  # Fichier setting dans un dataframe\n",
    "            self.setting_sheets = self.settings.sheet_names # List des onglets de setting\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def import_csv(self, filename, engine_val=None, encoding_val='utf-8', sep_val=';',low_memory_val=False):\n",
    "        self.source[filename] = pd.read_csv(r'Data/'+ filename + '.csv', engine=engine_val, encoding=encoding_val, sep=sep_val, low_memory=low_memory_val)\n",
    "        return self.source[filename]\n",
    "    \n",
    "    def import_xlsx(self, filename, sheetname=0):\n",
    "        self.source[filename] = pd.read_excel(r'Data/'+ filename + '.xlsx', sheet_name = sheetname)\n",
    "        return self.source[filename]\n",
    "    \n",
    "    def update_key(self, changed_keys, filename):\n",
    "        self.changed_keys[filename] = changed_keys\n",
    "        self.source[filename].rename(columns=changed_keys , inplace=True)\n",
    "        return self.source[filename]\n",
    "    \n",
    "    def import_dim(self,dimname, dimdataframe):\n",
    "        self.dim[dimname]=dimdataframe\n",
    "        \n",
    "    def import_fact(self,factname, factdataframe):\n",
    "        self.fact[factname] = factdataframe\n",
    "    \n",
    "    def export(self):\n",
    "        for key, value in self.dim.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-8', index=False)\n",
    "            \n",
    "        for key, value in self.fact.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-16', index=False)\n",
    "            \n",
    "    def export_audit(self):\n",
    "        for key, value in self.dim.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.xlsx'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_excel(location, index=False)\n",
    "            \n",
    "        for key, value in self.fact.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.xlsx'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_excel(location, index=False)\n",
    "            \n",
    "    def import_data(self):\n",
    "# Fonction générant le fichier setting lors du premier chargement si ce dernier n'existe pas\n",
    "# Etape 1 lecture des fichiers présents dans data et extraction des colonnes ainsi que des informations liées à la qualité de données\n",
    "# Etape 2 vérification si le fichier Settings.xlsx existe, s'il existe, il faut éventuellement le modifier avec les nouvelles informations, sinon le créer\n",
    "        \n",
    "        df_files_col = pd.DataFrame()\n",
    "        p = 1\n",
    "\n",
    "        #Etape 1 lecture des fichiers présents dans data et extraction des colonnes ainsi que des informations liées à la qualité de données\n",
    "        for key, value in self.source.items():\n",
    "            df = value\n",
    "\n",
    "            for i, col in enumerate(df.columns):              \n",
    "                df_files_col.at[p, 'Nom fichier source'] = key\n",
    "                df_files_col.at[p,'Nom champ source'] = col\n",
    "                df_files_col.at[p,'Type'] = str(df[col].dtypes)\n",
    "                df_files_col.at[p,'Synthèse'] = str(df[col].describe())\n",
    "                df_files_col.at[p, \"Date ajout\"] = str(datetime.date(datetime.now()))\n",
    "                df_files_col.at[p, \"Date modification\"] = str(datetime.date(datetime.now()))\n",
    "                \n",
    "                if i != 0:\n",
    "                    p = p + i\n",
    "                else:\n",
    "                    p += 1\n",
    "\n",
    "        df_dir_files = df_files_col.copy()\n",
    "        df_dir_files = df_dir_files[[\"Nom fichier source\", \"Date ajout\", \"Date modification\"]].drop_duplicates()\n",
    "\n",
    "        if path.exists(\"Settings & documentation\\Settings.xlsx\"):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='xlsxwriter')\n",
    "            workbook  = writer.book\n",
    "            df_dir_files.to_excel(writer, sheet_name='Fichiers Source', index=False)\n",
    "            df_files_col[['Nom fichier source', 'Nom champ source', 'Type', 'Synthèse']].to_excel(writer, sheet_name='Fichiers et colonnes source', index=False)\n",
    "\n",
    "            worksheet_FCS = writer.sheets[\"Fichiers et colonnes source\"]\n",
    "            worksheet_FS = writer.sheets[\"Fichiers Source\"]\n",
    "            #worksheet1 = workbook.add_worksheet(\"Mapping données\")\n",
    "\n",
    "            cell_format_FCS = workbook.add_format() \n",
    "            cell_format_FCS.set_text_wrap()\n",
    "            cell_format_FCS.set_align('center')\n",
    "            cell_format_FCS.set_align('vcenter')\n",
    "\n",
    "            cell_format_FS = workbook.add_format()    \n",
    "            cell_format_FS.set_align('left')\n",
    "\n",
    "            worksheet_FCS.set_column('A:B', 30, cell_format_FCS)\n",
    "            worksheet_FCS.set_column('C:C', 15, cell_format_FCS)\n",
    "            worksheet_FCS.set_column('D:D', 25, cell_format_FCS)\n",
    "            worksheet_FS.set_column('A:A', 40, cell_format_FS)\n",
    "            worksheet_FS.set_column('B:B', 20, cell_format_FS)\n",
    "            worksheet_FS.set_column('C:C', 20, cell_format_FS)\n",
    "            writer.save()\n",
    "            workbook.close()\n",
    "        self.settings = pd.ExcelFile(r\"Settings & documentation\\Settings.xlsx\")  # Fichier setting dans un dataframe\n",
    "        self.setting_sheets = self.settings.sheet_names # List des onglets de setting\n",
    "    def add_sheet_mapcol(self,DicDataframe=None):\n",
    "# Fonction créant la feuille Mapping dim dans le fichier Settings\n",
    "# Cette feuille contient l'ensemble des valeurs de dimension, elle permet de réaliser un mapping pour changer le nom des attributs\n",
    "\n",
    "        if DicDataframe == None:\n",
    "            DicDataframe = self.dim           \n",
    "            \n",
    "        df = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"],data=[])\n",
    "\n",
    "        for key, value in DicDataframe.items():        \n",
    "\n",
    "            for i, col in enumerate(value.columns):\n",
    "                df.at[i, \"Nom dimension\"] = key\n",
    "                df.at[i, \"Nom colonne\"] = col\n",
    "                df.at[i, \"Nouveau nom\"] = \"\"\n",
    "                df.at[i, \"A mapper\"] = \"Non\"                \n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        df.to_excel(writer, sheet_name=\"Mapping dim colonne\",engine='openpyxl',index=False)     \n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")\n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"A\"].width = 20\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"B\"].width = 20\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"C\"].width = 20\n",
    "\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "    \n",
    "    def map_col(self):        \n",
    "# Fonction transformant le nom des colonnes suivant le mapping effectué dans l'onglet Mapping dim colonne du fichier setting\n",
    "# Entrée: Dictionnaire key: Nom dataframe  Value : Dataframe\n",
    "# Sortie un dictionnaire \"nom\"/Dataframe avec les valeurs des colonnes actualisées suivant l'onglet de mapping \"Mapping dim colonne\" du fichier settings\n",
    "\n",
    "        dict_dataframe = self.dim\n",
    "        md = pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name=\"Mapping dim colonne\")\n",
    "        mdf = md[\"Nom dimension\"].unique()\n",
    "        dic_map = {}\n",
    "        \n",
    "        for dim in mdf:\n",
    "            map = md.loc[(md[\"Nom dimension\"] == dim ) & (md[\"Nouveau nom\"].notnull()== True),[\"Nom colonne\",\"Nouveau nom\"]]\n",
    "            map.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            for i, val in enumerate(map.iterrows()):           \n",
    "                dict_dataframe[dim].rename(columns={map.at[i, \"Nom colonne\"]:map.at[i,\"Nouveau nom\"]},inplace=True)\n",
    "\n",
    "        my_dim = dict_dataframe    \n",
    "    \n",
    "    def export_to_settings(self, dataframe, sheetname, position = None):\n",
    "# Fonction permettant d'exporter un dataframe vers un onglet de setting avec un formatage automatique des colonnes\n",
    "# Paramètres: \n",
    "# dataframe: correspond au dataframe à importer,\n",
    "# sheetname: correspond au nom de la feuille cible qui sera remplacées,\n",
    "# position: permet de choisir la position de la feuille modifiée dans le fichier excel\n",
    "   \n",
    "        letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        numbers = range(0,26,1)\n",
    "        dic_letters = {} # Dictionnaire contenant les lettres de l'alphabet    \n",
    "        val_max = [] # Liste contenant la taille des valeurs les plus longues\n",
    "\n",
    "        # Création d'un dictionnaire stockant les lettres de l'alphabet et leur position afin d'alimenter les fonctions openpyxl\n",
    "        for i in range(len(numbers)): \n",
    "            dic_letters[numbers[i]] = letters[i]\n",
    "\n",
    "        # Alimentation de val_max\n",
    "        for col in dataframe.columns:\n",
    "            max = (dataframe[col].astype(str)).str.len().max()        \n",
    "            if max != 0:\n",
    "                val_max.append(int(max)+1)\n",
    "            else:\n",
    "                val_max.append(15)\n",
    "\n",
    "       # Ouverture du fichier setting et création de l'onglet sheetname\n",
    "    \n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        list_sheet = workbook1.get_sheet_names() # Récupère la liste des feuilles\n",
    "\n",
    "        if sheetname in list_sheet: # Si l'onglet existe déjà on va le supprimer pour le remplacer\n",
    "            std = workbook1.get_sheet_by_name(sheetname)\n",
    "            workbook1.remove_sheet(std)\n",
    "\n",
    "        dataframe.to_excel(writer, sheet_name= sheetname, engine='openpyxl', index=False)     \n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        # Gère la position de la feuille\n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "\n",
    "        if position == None:        \n",
    "            pos = len(list_sheet) - 1\n",
    "        else:\n",
    "            pos = position\n",
    "\n",
    "        sheets = workbook1._sheets\n",
    "\n",
    "        sheet = sheets.pop(len(list_sheet) - 1)\n",
    "        sheets.insert(pos, sheet)\n",
    "\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "       # Réouverture du fichier setting\n",
    "    \n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "\n",
    "        # Alimentation du fichier excel avec les données du dataframe\n",
    "\n",
    "        for i, width in enumerate(val_max):\n",
    "            if width < 11:\n",
    "                workbook1[sheetname].column_dimensions[dic_letters[i]].width = 12\n",
    "            else:\n",
    "                workbook1[sheetname].column_dimensions[dic_letters[i]].width = width\n",
    "\n",
    "        # Sauvegarde et fermeture du fichier excel\n",
    "        \n",
    "        writer.save()\n",
    "        writer.close()    \n",
    "    \n",
    "    def mapp_data(self):\n",
    "        result = pd.DataFrame()\n",
    "        test=1\n",
    "\n",
    "        try:       \n",
    "            to_mapp_data = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping données\") # Lecture de la feuille Mapping données\n",
    "            to_mapp_data = to_mapp_data.loc[to_mapp_data[\"Valeurs cible\"].isna() == False]\n",
    "            to_mapp_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if to_mapp_data.empty == False:\n",
    "                for i, val in enumerate(to_mapp_data.iterrows()):\n",
    "                    self.dim[to_mapp_data.at[i,\"Table\"]][to_mapp_data.at[i,\"Colonnes\"]].replace(to_replace =to_mapp_data.at[i,\"Valeurs actuelles\"], \n",
    "                     value = to_mapp_data.at[i,\"Valeurs cible\"], inplace=True)\n",
    "        except:\n",
    "                mapp_col = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")\n",
    "                if mapp_col.empty == False:\n",
    "                    mapp_col = mapp_col.loc[mapp_col[\"A mapper\"] == \"Oui\", [\"Nom dimension\", \"Nom colonne\", \"Nouveau nom\"]]\n",
    "                    mapp_col[\"Colonne\"] = \"\"\n",
    "                    mapp_col.reset_index(inplace=True, drop=True)\n",
    "                    mapp_val = pd.DataFrame(columns=[\"Table\", \"Colonnes\", \"Valeurs actuelles\", \"Valeurs cible\"])\n",
    "                    p=1\n",
    "                    if mapp_col.empty == False:\n",
    "                        for i, val in enumerate(mapp_col.iterrows()):\n",
    "                            if str(mapp_col.at[i, \"Nouveau nom\"]) != \"nan\":\n",
    "                                mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nouveau nom\"]\n",
    "                            else:\n",
    "                                 mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nom colonne\"]\n",
    "\n",
    "                        for i,val in enumerate(mapp_col[[\"Nom dimension\", \"Colonne\"]].iterrows()):\n",
    "\n",
    "                            if (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).empty == False:\n",
    "                                list_val = (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).unique()\n",
    "                                nom_col = mapp_col.at[i, \"Colonne\"]\n",
    "                                nom_table = mapp_col.at[i, \"Nom dimension\"]\n",
    "\n",
    "                            if len(list_val) != 0:\n",
    "                                for i, val in enumerate(list_val):                           \n",
    "                                    mapp_val.at[p,\"Table\"] = nom_table\n",
    "                                    mapp_val.at[p,\"Colonnes\"] = nom_col\n",
    "                                    mapp_val.at[p,\"Valeurs actuelles\"] = val\n",
    "                                    mapp_val.at[p,\"Valeurs cible\"] = \"\"\n",
    "                                    p += 1\n",
    "\n",
    "                    if mapp_val.empty: \n",
    "                        pass            \n",
    "                    else:\n",
    "                        self.export_to_settings(mapp_val,\"Mapping données\")\n",
    "\n",
    "    def __compare__(self,sheet_name, cible_dataframe, excl_col= [], position=None):        \n",
    "        source = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=sheet_name)\n",
    "        source[\"compare\"] = \"\"\n",
    "        cible = cible_dataframe\n",
    "        cible[\"compare\"] = \"\"\n",
    "        colonnes_compared= [col for col in list(source.columns) if col not in excl_col]\n",
    "        \n",
    "        for col in colonnes_compared:\n",
    "            source[\"compare\"] = source[\"compare\"] + source[col].astype('str')        \n",
    "        \n",
    "        for col in colonnes_compared:\n",
    "            cible[\"compare\"] = cible[\"compare\"] + cible[col].astype('str')\n",
    "        \n",
    "        to_keep = [line for line in list(source[\"compare\"]) if line in list(cible[\"compare\"])]\n",
    "        \n",
    "        \n",
    "        to_add = [line for line in list(cible[\"compare\"]) if line not in list(source[\"compare\"])]\n",
    "        \n",
    "        result = source.loc[source[\"compare\"].isin(to_keep)]\n",
    "\n",
    "        \n",
    "        if to_add != \"\":\n",
    "            result = result.append(cible.loc[cible[\"compare\"].isin(to_add)])\n",
    "            \n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        result.drop([\"compare\"], axis=1, inplace=True)\n",
    "        return result\n",
    "        #self.export_to_settings(result,sheet_name,position)\n",
    "    \n",
    "    def update_fichier_source(self):\n",
    "        #Fonction mettant à jour l'onglet \"Fichiers Source\" du fichier Setting\n",
    "        \n",
    "        df_files_col = pd.DataFrame()\n",
    "        p = 1\n",
    "\n",
    "        #lecture des fichiers présents dans data et extraction des noms des fichiers     \n",
    "        for key, value in self.source.items():\n",
    "            \n",
    "            df = value\n",
    "\n",
    "            for i, col in enumerate(df.columns):\n",
    "                df_files_col.at[p, 'Nom fichier source'] = key\n",
    "                df_files_col.at[p,'Nom champ source'] = col\n",
    "                df_files_col.at[p, \"Date ajout\"] = \"\"\n",
    "                df_files_col.at[p, \"Date modification\"] = \"\"\n",
    "                p = p + i\n",
    "\n",
    "        df_dir_files = df_files_col.copy()\n",
    "        df_dir_files = df_dir_files[[\"Nom fichier source\", \"Date ajout\", \"Date modification\"]].drop_duplicates()\n",
    "        result = self.__compare__(\"Fichiers Source\", df_dir_files, [\"Date ajout\", \"Date modification\"],0 )\n",
    "        result[\"Date ajout\"] = result[\"Date ajout\"].fillna(str(datetime.date(datetime.now())))\n",
    "        result[\"Date modification\"] = str(datetime.date(datetime.now()))\n",
    "        \n",
    "        self.export_to_settings(result,\"Fichiers Source\",0)\n",
    "        \n",
    "    def update_columns(self):\n",
    "        #Fonction mettant à jour l'onglet Mapping dim colonne suite à l'ajout des nouvelles dimensions\n",
    "        existing_colmap =  pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name='Mapping dim colonne') # Contient les valeurs de mapping dim colonne\n",
    "        result = pd.DataFrame()\n",
    "        if existing_colmap.empty == True:\n",
    "            pass\n",
    "        else:\n",
    "            fresh_col = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"], data=[]) # Contient l'ensemble des nouvelles valeurs pour Mapping dim colonne\n",
    "            existing_colmap[\"Comparaison\"] = existing_colmap[\"Nom dimension\"] + existing_colmap[\"Nom colonne\"] # Création de la colonne \"Comparaison\" servant à comparer les lignes\n",
    "            to_keep = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à conserver dans existing_colmap\n",
    "            to_add = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à rajouter\n",
    "            p=0\n",
    "\n",
    "            for key, value in self.dim.items():\n",
    "\n",
    "                for i, col in enumerate(value.columns):\n",
    "                    fresh_col.at[p, \"Nom dimension\"] = key\n",
    "                    fresh_col.at[p, \"Nom colonne\"] = col\n",
    "                    fresh_col.at[p, \"Nouveau nom\"] = \"\"\n",
    "                    fresh_col.at[p, \"A mapper\"] = \"Non\"\n",
    "                    p+=1\n",
    "\n",
    "            fresh_col[\"Comparaison\"] = fresh_col[\"Nom dimension\"] + fresh_col[\"Nom colonne\"]\n",
    "            to_keep = list(fresh_col[\"Comparaison\"])\n",
    "            existing_colmap = existing_colmap.loc[existing_colmap[\"Comparaison\"].isin(to_keep)]\n",
    "            existing_colmap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # On détermine la liste des nouvelles valeurs absentes de la feuille de mapping\n",
    "\n",
    "            for val in to_keep:\n",
    "                if val in list(existing_colmap[\"Comparaison\"]):\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    to_add.append(val)\n",
    "\n",
    "            fresh_col = fresh_col.loc[fresh_col[\"Comparaison\"].isin(to_add)]\n",
    "            fresh_col.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            result = existing_colmap.append(fresh_col)\n",
    "\n",
    "            # On exporte le résultat dans l'onglet Mapping dim colonne de Setting\n",
    "            result = result.drop([\"Comparaison\"], axis=1)\n",
    "            result = result.sort_values([\"Nom dimension\", \"Nom colonne\"])\n",
    "            self.export_to_settings(result, \"Mapping dim colonne\",2)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def update_map_val(self):       \n",
    "        mapp_col = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")\n",
    "        mapp_col = mapp_col.loc[mapp_col[\"A mapper\"] == \"Oui\", [\"Nom dimension\", \"Nom colonne\", \"Nouveau nom\"]]\n",
    "        mapp_col[\"Colonne\"] = \"\"\n",
    "        mapp_col.reset_index(inplace=True, drop=True)\n",
    "        mapp_val = pd.DataFrame(columns=[\"Table\", \"Colonnes\", \"Valeurs actuelles\", \"Valeurs cible\"])\n",
    "        p=1\n",
    "        if mapp_col.empty == False:\n",
    "            for i, val in enumerate(mapp_col.iterrows()):\n",
    "                if str(mapp_col.at[i, \"Nouveau nom\"]) != \"nan\":\n",
    "                    mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nouveau nom\"]\n",
    "                else:\n",
    "                     mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nom colonne\"]        \n",
    "\n",
    "            for i,val in enumerate(mapp_col[[\"Nom dimension\", \"Colonne\"]].iterrows()):\n",
    "\n",
    "                if (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).empty == False:\n",
    "                    list_val = (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).unique()\n",
    "                    nom_col = mapp_col.at[i, \"Colonne\"]\n",
    "                    nom_table = mapp_col.at[i, \"Nom dimension\"]\n",
    "\n",
    "                if len(list_val) != 0:\n",
    "                    for i, val in enumerate(list_val):                           \n",
    "                        mapp_val.at[p,\"Table\"] = nom_table\n",
    "                        mapp_val.at[p,\"Colonnes\"] = nom_col\n",
    "                        mapp_val.at[p,\"Valeurs actuelles\"] = val\n",
    "                        mapp_val.at[p,\"Valeurs cible\"] = \"\"\n",
    "                        p += 1\n",
    "                        \n",
    "        if mapp_val.empty: \n",
    "            pass     \n",
    "        else:\n",
    "            result = self.__compare__(\"Mapping données\", mapp_val,excl_col=[\"Valeurs cible\"])\n",
    "            self.export_to_settings(result,\"Mapping données\",3)\n",
    "            \n",
    "    def add_col_group(self, regroup_name, dim_source, col_source):\n",
    "            \n",
    "        target_sheet = pd.DataFrame(columns=[\"Dimension source\", \"Colonne source\", \"Valeurs sources\", \"Valeurs cibles\"], data=[])\n",
    "        export_name =  \"Gr_\" + dim_source + \"_\" + regroup_name \n",
    "        \n",
    "        # Préparation du dataframe target_sheet\n",
    "        \n",
    "        for i, val in enumerate(list(self.dim[dim_source][col_source].unique())):\n",
    "            target_sheet.at[i, \"Dimension source\"] = dim_source\n",
    "            target_sheet.at[i, \"Colonne source\"] = col_source\n",
    "            target_sheet.at[i, \"Valeurs sources\"] = val\n",
    "        \n",
    "        if export_name in self.setting_sheets:\n",
    "            result = self.__compare__(export_name, target_sheet, excl_col= [\"Valeurs cibles\"])\n",
    "            self.export_to_settings(result,export_name)\n",
    "            replaced_values = list(result.loc[result[\"Valeurs cibles\"].isna() == False, \"Valeurs sources\"])\n",
    "            replacing_values = list(result.loc[result[\"Valeurs cibles\"].isna() == False, \"Valeurs cibles\"])\n",
    "            self.dim[dim_source][export_name] = self.dim[dim_source][col_source]\n",
    "            self.dim[dim_source][export_name].replace(replaced_values, replacing_values, inplace=True)\n",
    "        else:\n",
    "            self.export_to_settings(target_sheet, export_name)                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation de la classe Datamanagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Datamanagement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Ajouter les tables à intégrer ici\n",
    "temps = my_data.import_xlsx(\"Dim temps\")\n",
    "offres = my_data.import_xlsx('PBI_Offres')\n",
    "candidature = my_data.import_xlsx('PBI_Candidatures')\n",
    "#tcw = my_data.import_csv('191119_TrainingCollectiveWishes') # Demandes collectives non affectées à un plan de formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation du fichier setting ou update du fichier setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:201: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:205: DeprecationWarning: Call to deprecated function remove_sheet (Use wb.remove(worksheet) or del wb[sheetname]).\n"
     ]
    }
   ],
   "source": [
    "my_data.import_data()\n",
    "my_data.update_fichier_source()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transformation des données sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Remarques: Les données offres comportent des doublons. Une offre est publiée par exemple plusieurs fois le même jour, parfois même à la même minute.\n",
    "# Des règles sont appliquées afin de rendre unique la combinaison IdDemandeAuto, Date de publication de l'offre, Date de clôture réelle\n",
    "\n",
    "# Ajout du champ date de candidature contenant uniquement la partie date du champ Date de publication de l'offre\n",
    "offres[\"date de publication\"] = offres[\"Date de publication de l'offre\"].dt.date\n",
    "\n",
    "# Suppression du champ \"Actuellement publié\" qui dédoublonne les lignes inutilement\n",
    "offres = offres.drop([\"Actuellement publié\"], axis=1)\n",
    "\n",
    "#Suppression des lignes doublons\n",
    "offres = offres.drop_duplicates()\n",
    "\n",
    "# Suppression des Date de publication identiques (on conserve pour un même jour que l'heure la plus récente ): regroupement des données par \"IdDemandeAuto\",\"date candidature\" et max \"Date de publication de l'offre\" afin ne conserver que les candidatures les plus récentes chauque jour\n",
    "to_keep = offres.groupby([\"IdDemandeAuto\",\"date de publication\"])[\"Date de publication de l'offre\"].max().reset_index()\n",
    "to_keep = to_keep.merge(offres[[\"IdDemandeAuto\", \"date de publication\",\"Date de publication de l'offre\",\"Date de clôture réelle\"]], on=[\"IdDemandeAuto\",\"date de publication\",\"Date de publication de l'offre\"], how=\"left\")\n",
    "\n",
    "# Lorsque date et heure de candidature identiques, on conserve la ligne avec la date de clôture réelle la plus grande\n",
    "to_keep = to_keep.groupby([\"IdDemandeAuto\",\"date de publication\",\"Date de publication de l'offre\"])[\"Date de clôture réelle\"].max().reset_index()\n",
    "\n",
    "# Jointure avec la table offres afin de récupérer les autres données\n",
    "offres = to_keep.merge(offres, on=[\"IdDemandeAuto\",\"date de publication\",\"Date de publication de l'offre\",\"Date de clôture réelle\"], how=\"left\")\n",
    "offres.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction indicator permet de rattacher un évènement temporel (changement de statut, date de publication) à la dernière date du mois durant laquelle l'évènement s'est produit.\n",
    "# Ceci permet de réaliser des jointures dans la table de fait pour rattacher l'évènement à une ligne.\n",
    "\n",
    "def indicator_dt(dataframe_source, champ_source, nom_champ_cible):\n",
    "    dataframe_source[nom_champ_cible] = dataframe_source[champ_source].apply(lambda x: x.replace(day=x.days_in_month))\n",
    "    dataframe_source[nom_champ_cible] = dataframe_source[nom_champ_cible].astype(str)\n",
    "    dataframe_source[nom_champ_cible] = dataframe_source[nom_champ_cible].apply(lambda x: x[:10])\n",
    "    \n",
    "\n",
    "indicator_dt(offres, \"Date de publication de l'offre\", \"Ind dt publication de l'offre\")\n",
    "indicator_dt(offres, \"Date de clôture réelle\", \"Ind clôture réelle\")\n",
    "indicator_dt(offres, \"Date d'ouverture\", \"Ind date d'ouverture\")\n",
    "indicator_dt(offres, \"Date de fermeture\", \"Ind date de fermeture\")\n",
    "offres[\"custom\"] = 1\n",
    "\n",
    "\n",
    "offres[\"publication\"] = 1\n",
    "offres[\"clôture\"] = 1\n",
    "offres[\"ouverture\"] = 1\n",
    "offres[\"fermeture\"] = 1\n",
    "\n",
    "\n",
    "my_data.import_dim(\"offres_test\", offres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_dt(candidature,\"Date du dépôt de candidature\" , \"Ind dépôt candidature\")\n",
    "candidature[\"candidature\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Creation des tables de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table dimension offres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2017-11-30 14:33:00\n",
      "1       2017-11-30 14:33:00\n",
      "2       2017-12-31 14:33:00\n",
      "3       2017-11-30 08:44:00\n",
      "4       2018-02-28 08:48:00\n",
      "                ...        \n",
      "22975   2017-12-31 09:29:00\n",
      "22976   2018-01-31 13:57:00\n",
      "22977   2018-03-31 13:57:00\n",
      "22978   2018-04-30 13:57:00\n",
      "22979   2014-12-31 13:59:00\n",
      "Name: Date de publication de l'offre, Length: 22980, dtype: datetime64[ns]\n",
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Génération de la table de dimension offre contenant les attributs d'offre n'évoluant peu\n",
    "\n",
    "# Etape 1: Sélection des champs concernés qui seront stockés dans un DataFrame (équivalent d'une table)\n",
    "dim_offres = offres[[\"IdDemandeAuto\",\"Pays\",\"Domaine professionnel\",\"Branche\",\"Recrutement interne/ externe\",\"Lieu de travail\",\"Intitulé du poste\",\"Langue\",\"Employeur (si connu)\",\"Région\",\"NP\",\"Type d'emploi\",\"Niveau de formation\",\"EO\",\"niveau d’expérience requis)\",\"Line Manager\",\n",
    "                     \"Talent Developer\",\"Bibliothèque d'emplois\",\"Période d'arrivée souhaitée\",\"Localisation (Précisions/Mots-clés)\",\"Nom complet\",\"Mail Adresse TD\",\"Adresse mail equipe d'offre\",\"Type de contrat\",\"Contrat rotationnel\",\"Marque\"]].copy() \n",
    "\n",
    "print(offres[\"Date de publication de l'offre\"].apply(lambda x: x.replace(day=x.days_in_month))) # Récupération du dernier jour du mois\n",
    "\n",
    "# Etape 2: Suppression des doublons et réinitialisation de l'index\n",
    "dim_offres.drop_duplicates(inplace=True)\n",
    "dim_offres.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Etape 3: Ajout d'une clé technique\n",
    "dim_offres[\"Key_offres\"] = dim_offres.index\n",
    "my_data.import_dim(\"dim_offres\", dim_offres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la dimension temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Création de la table de temps. Nous gardons que les 2 dernières années\n",
    "dim_temps =  temps[[\"date key\",\"full date\", \"day num in month\", \"month\", \"month name\", \"year\"]].copy()\n",
    "\n",
    "# Détermination de l'année actuelle\n",
    "now = dt.datetime.now()\n",
    "actual_year = now.year\n",
    "now = np.datetime64(now)\n",
    "\n",
    "# dim_temps est filtrée sur l'année courante et l'année précédente\n",
    "dim_temps = dim_temps.loc[(dim_temps[\"year\"]==actual_year) | (dim_temps[\"year\"] == actual_year-1) | (dim_temps[\"year\"] == actual_year-2)]\n",
    "\n",
    "# Les dates portants sur le furtur ne sont pas retenues sur l'année en cours\n",
    "dim_temps = dim_temps.loc[dim_temps[\"full date\"] <= now ]\n",
    "my_data.import_dim(\"Temps\", dim_temps)\n",
    "\n",
    "# Création de la table fact_temps permettant de créer la table de fait\n",
    "fact_temps = dim_temps[[\"year\", \"month\",\"day num in month\"]]\n",
    "fact_temps = fact_temps.groupby([\"year\", \"month\"])[\"day num in month\"].max().reset_index()\n",
    "\n",
    "fact_temps = fact_temps.merge(dim_temps[[\"year\", \"month\", \"day num in month\", \"full date\", \"date key\"]], how='left', on=[\"year\", \"month\",\"day num in month\"])\n",
    "fact_temps[\"custom\"] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement du mapping des colonnes et des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 296 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:201: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "C:\\Users\\sgasmi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:205: DeprecationWarning: Call to deprecated function remove_sheet (Use wb.remove(worksheet) or del wb[sheetname]).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Verification si la colonne Mapping dim existe dans le fichier setting, si non on va la créer\n",
    "try:\n",
    "    pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")   \n",
    "    Mapp_sheet = True    \n",
    "except:\n",
    "    Mapp_sheet = False\n",
    "    \n",
    "    \n",
    "if Mapp_sheet == True:\n",
    "    my_data.update_columns()\n",
    "    pass\n",
    "else:    \n",
    "    #add_sheet_mapcol(my_data.dim)\n",
    "    my_data.add_sheet_mapcol()\n",
    "\n",
    "my_data.map_col()\n",
    "#my_data.dim = map_col(my_data.dim)\n",
    "\n",
    "my_data.mapp_data()\n",
    "\n",
    "try:\n",
    "    pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping données\")   \n",
    "    Mapp_val = True    \n",
    "except:\n",
    "    Mapp_val = False\n",
    "    \n",
    "if Mapp_val == True :\n",
    "    my_data.update_map_val()\n",
    "\n",
    "#my_data.add_col_group(\"group1\", \"Plan de formation\", \"planname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Création des tables de fait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de faits offres d'emplois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Création de la table de fait offres d'emploi: Grain = 1 offre par mois\n",
    "\n",
    "fact_offres = offres[[\"IdDemandeAuto\",\"custom\"]].copy()\n",
    "fact_offres = fact_offres.drop_duplicates()\n",
    "fact_offres = fact_offres.merge(fact_temps[[\"date key\",\"full date\",\"custom\"]], on=[\"custom\"], how = 'outer')\n",
    "fact_offres[\"full date\"] = fact_offres[\"full date\"].astype(str) # Full date en string pour jointure\n",
    "fact_offres = fact_offres.merge(dim_offres[[\"IdDemandeAuto\",\"Key_offres\"]], on=[\"IdDemandeAuto\"], how='left') # Ajout de la clé technique Key_offres\n",
    "\n",
    "\n",
    "# Calcul indicateur publication\n",
    "fact_offres = fact_offres.merge(offres[[\"IdDemandeAuto\",\"Ind dt publication de l'offre\", \"publication\"]], left_on=[\"IdDemandeAuto\", \"full date\"],right_on=[\"IdDemandeAuto\",\"Ind dt publication de l'offre\"], how='left')\n",
    "fact_offres = fact_offres.groupby([\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"])[\"publication\"].count().reset_index()\n",
    "\n",
    "# Calcul indicateur first publication\n",
    "stg_first_pub = fact_offres.loc[fact_offres[\"publication\"]!=0]\n",
    "stg_first_pub = stg_first_pub[[\"IdDemandeAuto\",\"full date\"]].groupby(\"IdDemandeAuto\")[\"full date\"].min().reset_index()\n",
    "stg_first_pub[\"first publication\"] = \"1\"\n",
    "stg_first_pub[\"first publication\"] = stg_first_pub[\"first publication\"].astype(int)\n",
    "fact_offres = fact_offres.merge(stg_first_pub, on=[\"IdDemandeAuto\",\"full date\"], how=\"left\")\n",
    "\n",
    "\n",
    "# Calcal indicateur clôture réelle\n",
    "\n",
    "fact_cloture = fact_offres.merge(offres[[\"IdDemandeAuto\",\"Ind clôture réelle\", \"clôture\"]], left_on=[\"IdDemandeAuto\", \"full date\"], right_on=[\"IdDemandeAuto\",\"Ind clôture réelle\"], how ='left')\n",
    "\n",
    "fact_cloture = fact_cloture.groupby([\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"])[\"clôture\"].count().reset_index()\n",
    "\n",
    "fact_offres = fact_offres.merge(fact_cloture, on = [\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"], how='left')\n",
    "\n",
    "# Ajout du champ fist virtual publication visant à identifier toutes les premières publication, et ajouter celles nécessaires pour les offres n'ayant qu'une date de clôture\n",
    "\n",
    "fact_offres[\"first virtual publication\"] = fact_offres[\"first publication\"]\n",
    "\n",
    "list_no_fpublication = fact_offres[[\"IdDemandeAuto\", \"first publication\"]].groupby([\"IdDemandeAuto\"])[\"first publication\"].count().reset_index()\n",
    "\n",
    "list_no_fpublication = list_no_fpublication.loc[list_no_fpublication[\"first publication\"]==0, \"IdDemandeAuto\"]\n",
    "\n",
    "list_no_fpublication = list(list_no_fpublication)\n",
    "\n",
    "list_one_cloture = fact_offres[[\"IdDemandeAuto\", \"clôture\"]].groupby([\"IdDemandeAuto\"])[\"clôture\"].sum().reset_index()\n",
    "\n",
    "list_one_cloture = list_one_cloture.loc[list_one_cloture[\"clôture\"]>= 1, \"IdDemandeAuto\"]\n",
    "\n",
    "list_one_cloture = list(list_one_cloture)\n",
    "\n",
    "\n",
    "first_date =  fact_temps.at[0, \"full date\"]\n",
    "first_date = str(first_date)[:10]\n",
    "\n",
    "fact_offres.loc[(fact_offres[\"IdDemandeAuto\"].isin(list_no_fpublication)) & (fact_offres[\"full date\"] == first_date) & (fact_offres[\"IdDemandeAuto\"].isin(list_one_cloture)), \"first virtual publication\"]=1\n",
    "\n",
    "# Ajout du témoin de statut\n",
    "\n",
    "#1 Classement des offres par IdDeamndeAutot et full date\n",
    "fact_offres = fact_offres.sort_values(by=[\"IdDemandeAuto\", \"full date\"])\n",
    "list_offre = list(fact_offres[\"IdDemandeAuto\"].unique())\n",
    "\n",
    "#2 Somme de publication et clôture\n",
    "fact_offres[\"Sum\"] = fact_offres[\"publication\"] + fact_offres[\"clôture\"]\n",
    "for i, val in enumerate(fact_offres.iterrows()):\n",
    "    if fact_offres.at[i, \"first virtual publication\"] == 1 and fact_offres.at[i, \"publication\"] == 0:\n",
    "        fact_offres.at[i, \"Sum\"] += 1\n",
    "        \n",
    "#3 Cumul des sommes\n",
    "fact_offres['Cumul'] = fact_offres.groupby(['IdDemandeAuto'])['Sum'].cumsum()\n",
    "\n",
    "#4 Check pour chaque cumul si il est paire ou impore. Si cumul est impaire alors l'offre est publiée sinon non.\n",
    "fact_offres[\"publié\"] = fact_offres[\"Cumul\"]\n",
    "fact_offres[\"publié\"] = fact_offres[\"publié\"].apply(lambda x: x if x!=0 else 0)\n",
    "fact_offres[\"publié\"] = fact_offres[\"publié\"].apply(lambda x: 0 if int(x)% 2== 0 else 1)\n",
    "\n",
    "# Nombre d'ouverture d'offres\n",
    "fact_ouv = fact_offres.merge(offres[[\"IdDemandeAuto\",\"Ind date d'ouverture\", \"ouverture\"]], left_on=[\"IdDemandeAuto\", \"full date\"],right_on=[\"IdDemandeAuto\",\"Ind date d'ouverture\"], how='left')\n",
    "\n",
    "fact_ouv = fact_ouv.groupby([\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"])[\"ouverture\"].nunique().reset_index()\n",
    "\n",
    "fact_offres = fact_offres.merge(fact_ouv, on = [\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"], how='left')\n",
    "\n",
    "# Nombre de fermeture d'offres\n",
    "fact_fer = fact_offres.merge(offres[[\"IdDemandeAuto\",\"Ind date de fermeture\", \"fermeture\"]], left_on=[\"IdDemandeAuto\", \"full date\"],right_on=[\"IdDemandeAuto\", \"Ind date de fermeture\"], how='left')\n",
    "\n",
    "fact_fer = fact_fer.groupby([\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"])[\"fermeture\"].nunique().reset_index()\n",
    "\n",
    "fact_offres = fact_offres.merge(fact_fer, on = [\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"], how='left')\n",
    "\n",
    "# Nombre d'ouverture virtuelles\n",
    "fact_offres[\"virtual ouverture\"] = fact_offres[\"ouverture\"]\n",
    "\n",
    "list_no_fouverture = fact_offres[[\"IdDemandeAuto\", \"ouverture\"]].groupby([\"IdDemandeAuto\"])[\"ouverture\"].sum().reset_index()\n",
    "\n",
    "list_no_fouverture = list_no_fouverture.loc[list_no_fouverture[\"ouverture\"]==0, \"IdDemandeAuto\"]\n",
    "\n",
    "list_no_fouverture = list(list_no_fouverture)\n",
    "\n",
    "list_one_fermeture = fact_offres[[\"IdDemandeAuto\", \"fermeture\"]].groupby([\"IdDemandeAuto\"])[\"fermeture\"].sum().reset_index()\n",
    "\n",
    "list_one_fermeture = list_one_fermeture.loc[list_one_fermeture[\"fermeture\"]>= 1, \"IdDemandeAuto\"]\n",
    "\n",
    "list_one_fermeture = list(list_one_fermeture)\n",
    "\n",
    "fact_offres.loc[(fact_offres[\"IdDemandeAuto\"].isin(list_no_fouverture)) & (fact_offres[\"full date\"] == first_date) & (fact_offres[\"IdDemandeAuto\"].isin(list_one_fermeture)), \"virtual ouverture\"]=1\n",
    "\n",
    "# Cas des ouvertures dans le passé n'étant pas présent dans table de fait  +  pas de date de clôture dans table de fait\n",
    "nul_fermeture = offres.groupby([\"IdDemandeAuto\"])[\"Date de fermeture\"].count().reset_index()\n",
    "nul_fermeture = nul_fermeture.loc[nul_fermeture[\"Date de fermeture\"]==0]\n",
    "nul_fermeture = list(nul_fermeture[\"IdDemandeAuto\"])\n",
    "\n",
    "nul_ouverture = fact_offres.groupby([\"IdDemandeAuto\"])[\"virtual ouverture\"].sum().reset_index()\n",
    "nul_ouverture = nul_ouverture.loc[nul_ouverture[\"virtual ouverture\"]==0]\n",
    "nul_ouverture = list(nul_ouverture[\"IdDemandeAuto\"])\n",
    "\n",
    "nv_ouvert = [x for x in nul_fermeture if x in nul_ouverture ]\n",
    "fact_offres.loc[fact_offres[\"IdDemandeAuto\"].isin(nv_ouvert) & (fact_offres[\"full date\"] == first_date), \"virtual ouverture\"] = 1\n",
    "\n",
    "# Ajout du témoin de ouvert\n",
    "\n",
    "#1 Somme de publication et clôture\n",
    "fact_offres[\"Sum ouv\"] = fact_offres[\"virtual ouverture\"] + fact_offres[\"fermeture\"]\n",
    "        \n",
    "#2 Cumul des sommes\n",
    "fact_offres['Cumul ouv'] = fact_offres.groupby(['IdDemandeAuto'])['Sum ouv'].cumsum()\n",
    "\n",
    "#3 Check pour chaque cumul si il est paire ou impore. Si cumul est impaire alors l'offre est publiée sinon non.\n",
    "fact_offres[\"ouvert\"] = fact_offres[\"Cumul ouv\"]\n",
    "fact_offres[\"ouvert\"] = fact_offres[\"ouvert\"].apply(lambda x: x if x!=0 else 0)\n",
    "fact_offres[\"ouvert\"] = fact_offres[\"ouvert\"].apply(lambda x: 0 if int(x)% 2== 0 else 1)\n",
    "\n",
    "#Cas des offres ouvertes mais non fermée\n",
    "\n",
    "\n",
    "# indicateur nombre de candidatures\n",
    "fact_candidature = fact_offres.merge(candidature[[\"IdDemandeAuto\",\"Ind dépôt candidature\", \"candidature\"]], left_on=[\"IdDemandeAuto\", \"full date\"], right_on=[\"IdDemandeAuto\",\"Ind dépôt candidature\"], how ='left')\n",
    "fact_candidature = fact_candidature.groupby([\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"])[\"candidature\"].count().reset_index()\n",
    "fact_offres = fact_offres.merge(fact_candidature, on = [\"IdDemandeAuto\",\"date key\",\"full date\",\"Key_offres\"], how='left')\n",
    "\n",
    "# indicateur cumul candidatures\n",
    "fact_offres['Cumul candidature'] = fact_offres.groupby(['IdDemandeAuto'])['candidature'].cumsum()\n",
    "\n",
    "# indicateurs sans nombre d'offres sans candidatures\n",
    "\n",
    "fact_offres.loc[(fact_offres[\"Cumul candidature\"]== 0) & ((fact_offres[\"publié\"] ==1) | (fact_offres[\"ouvert\"]==1)), \"Nombre offres sans candidataures\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_data.import_fact(\"fact_offres\", fact_offres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de faits XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export des données vers le répertoire Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_data.export_audit()\n",
    "my_data.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(my_data.dim[\"Souhaits de formation\"][\"cspcode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
